\documentclass[12pt,A4]{extarticle}	
\usepackage{filecontents}

\begin{filecontents}{\jobname.bib}
  @article{DistributedSystemVanSteenTanenbaum,
  author       = {Maarten van Steen and
                  Andrew S. Tanenbaum},
  title        = {A brief introduction to distributed systems},
  journal      = {Computing},
  volume       = {98},
  number       = {10},
  pages        = {967--1009},
  year         = {2016},
  url          = {https://doi.org/10.1007/s00607-016-0508-7},
  doi          = {10.1007/s00607-016-0508-7},
  timestamp    = {Thu, 14 Oct 2021 09:12:09 +0200},
  biburl       = {https://dblp.org/rec/journals/computing/SteenT16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lamportTimeClocks,
author = {Lamport, Leslie},
title = {Time, clocks, and the ordering of events in a distributed system},
year = {1978},
issue_date = {July 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/359545.359563},
doi = {10.1145/359545.359563},
abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
journal = {Commun. ACM},
month = {jul},
pages = {558â€“565},
numpages = {8},
keywords = {clock synchronization, computer networks, distributed systems, multiprocess systems}
}
@article{BRACHA1987130,
title = {Asynchronous Byzantine agreement protocols},
journal = {Information and Computation},
volume = {75},
number = {2},
pages = {130-143},
year = {1987},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(87)90054-X},
url = {https://www.sciencedirect.com/science/article/pii/089054018790054X},
author = {Gabriel Bracha},
abstract = {A consensus protocol enables a system of n asynchronous processes, some of them faulty, to reach agreement. Both the processes and the message system are capable of cooperating to prevent the correct processes from reaching decision. A protocol is t-resilient if in the presence of up to t faulty processes it reaches agreement with probability 1. Byzantine processes are faulty processes that can deviate arbitrarily from the protocol; Fail-Stop processes can just stop participating in it. In a recent paper, t-resilient randomized consensus protocols were presented for t<n5. We improve this to t < n3, thus matching the known lower bound on the number of correct processes necessary for consensus. The protocol uses a general technique in which the behavior of the Byzantine processes is restricted by the use of a broadcast protocol that filters some of the messages. The apparent behavior of the Byzantine processes, filtered by the broadcast protocol, is similar to that of Fail-Stop processes. Plugging the broadcast protocol as a communicating primitive into an agreement protocol for Fail-Stop processes gives the result. This technique, of using broadcast protocols to reduce the power of the faulty processes and then using them as communication primitives in algorithms designed for weaker failure models, was used succesfully in other contexts.}
}
\end{filecontents}

\newcommand{\lectureTitle}{Decentralized Systems [WIP]}
\newcommand{\lectureSubtitle}{Fundamentals, Modeling, and Applications}
\newcommand{\semester}{Sommersemester 2024}

\newcommand{\titleSize}{\LARGE}

\input{../shared/summary-boilerplate.tex}
\cfoot{\thepage\ $/$ \pageref*{LastPage}}


\definecolor{highlightColor}{RGB}{66, 135, 245}
\newcommand{\highlight}[1]{\textcolor{highlightColor}{\textbf{#1}}}

\def\contentsname{\empty}

\begin{document}

\disclaimer

\tableofcontents
\clearpage

\section{Introduction}
\subsection{What is a distributed system?}
\subsubsection{Characteristics by van Steen and Tanenbaum}
``\textit{A distributed system is a collection of autonomous computing elements that appears to its users as a single coherent system.}'' (\cite{DistributedSystemVanSteenTanenbaum}), this results in two characteristics:
\begin{enumerate}
  \item{``\textbf{Collection of autonomous computing elements}'': ``\textit{In practice, nodes are programmed to achieve common goals, which are realized by exchanging messages with each other}'' \\(\cite{DistributedSystemVanSteenTanenbaum})}
  \item{``\textbf{Appears as a single coherent system}'': Appears as a single large system}
\end{enumerate}

\subsubsection{Aspects of characteristic 1}
\begin{itemize}
  \item{``\textit{we cannot assume that there is something like a global clock}'' (\cite{DistributedSystemVanSteenTanenbaum}), therefore the \textbf{synchronization and coordination} between participants must be worked out}
  \item{``\textit{The fact that we are dealing with a collection of nodes implies that we may also need to manage the membership and organization of that collection}'' (\cite{DistributedSystemVanSteenTanenbaum}), therefore we need to think about identities and possible access-restrictions}
\end{itemize}

\subsubsection{Aspects of characteristic 2}
\begin{itemize}
  \item{``\textit{To assist the development of distributed applications, distributed systems are often organized to have a separate layer of software that is logically placed on top of the respective operating systems of the computers that are part of the system [...] leading to what is known as middleware}'' (\cite{DistributedSystemVanSteenTanenbaum})}
\end{itemize}

\subsubsection{Observations}
\begin{itemize}
  \item{Distributing tasks and aggregating a result from them is not easy at all}
  \item{The coordination of those tasks is still \textbf{centralized}}
\end{itemize}

\subsection{What makes a distributed system a decentralized system?}
According to \textbf{ISO/TC 307}: ``\textit{distributed system wherein control is distributed among the persons or organizations participating in the operation of the system}''

\subsubsection{Three types of Decentralization (Vitalik Buterin)}
\href{https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274}{Vitalik Buterin defines three types of Decentralization}:
\begin{itemize}
  \item{\textbf{Architectural (de)centralization}: How many \textbf{physical computers} is a system made up of? How many of those computers can it tolerate breaking down at any single time?}
  \item{\textbf{Political (de)centralization}: How many \textbf{individuals or organizations} ultimately control the computers that the system is made up of?}
  \item{\textbf{Logical (de)centralization}: Does the \textbf{interface and data structures} that the system presents and maintains look more like a single monolithic object, or an amorphous swarm? One simple heuristic is: if you cut the system in half, including both providers and users, will both halves continue to fully operate as independent units?}
\end{itemize}

\subsubsection{Our definition of decentralized systems}
\begin{itemize}
  \item{A decentralized system has political decentralization, where multiples parties are making their own independent decisions (they can still coordinate with each other)}
  \item{If a system is architecturally but not politically decentralized, we call it a distributed system}
  \item{Decentralized systems can be \textbf{logically decentralized or centralized}}
  \item{Decentralized systems can be open systems (anybody can participate) or closed systems}
\end{itemize}

\subsection{Reasons for decentralization}
\subsubsection{Reasons for architectural decentralization}
\begin{itemize}
  \item{\textbf{Latency}}
  \item{\textbf{Scalability}: Scale number of machines running the system}
  \item{Increase \textbf{fault tolerance} and \textbf{availability}, remove single point of failures}
  \item{Increase \textbf{attack resistance}, because no central points exist}
\end{itemize}

\subsubsection{Reasons for political decentralization}
\begin{itemize}
  \item{\textbf{Collusion resistance}: It is harder for participants to collude in ways that benefit a small group at the expense of other participants}
  \item{\textbf{Power} can be distributed ``equally''}
\end{itemize}

\subsubsection{Reasons for logical decentralization}
Logical decentralization is not always possible or even wanted, especially in use cases we cover. Example: \textbf{Distributed Ledgers}, where the goal is to have one commonly agreed system state at any point in time.

\subsection{Challenges of decentralization}
Decentralized systems come with risks/challenges to avoid harm to the system or its participants:
\begin{itemize}
  \item{\textbf{Time and synchrony}: Do we have a global clock? Is the communication synchronous or asynchronous?}
  \item{\textbf{behavior} of nodes: Can we handle arbitrary behavior? How many faulty nodes can we tolerate?}
  \item{\textbf{Identity}: Do we have an open system? Are nodes (identities) known?}
\end{itemize}

\section{Fundamentals}
\subsection{How to model a distributed system?}
We define a \textbf{distributed system} as a set of identical processes (or processors) that execute a program. Coordination between the processors is needed. The combination of processes form an application.

\subsubsection{Processes (Processors) and Messages}
A distributed system or algorithm consists of $n$ \highlight{processors} (called nodes/agents/participants) $p_0, \dots, p_{n-1}$.\par
Each process runs a local process and the processors cooperate on some \underline{common} task. They can communicate with each other.\par
\highlight{Messages} are uniquely identified by the sender using a sequence number of a logical clock.

\subsubsection{Links}
A \highlight{link} (channel) $\{p_i, p_j\}$ connects processors $i$ and $j$. Links are always considered \textbf{bidirectional}.\par
The network is a collection of all channels, the tolopogy is a pattern of channels (e.g. mesh, star).

\subsubsection{Inter-Process Communication}
Processors are communicating by \textbf{passing messages} to \textbf{in-} and \textbf{outboxes}. The address is a set of processes. Processors have access to \highlight{shared memory}.

\subsubsection{Automata and Steps}
We can model a distributed algorithm as a distributed collection of \highlight{automata} (one per process). Each automata is a state machine with defined states (\textbf{configurations}) and state transitions (\textbf{step}) that are triggered by an \textbf{event}.

\subsubsection{Safety and Liveness}
\begin{itemize}
  \item{\highlight{Safety}: ``\textit{Nothing bad has happened, yet}'': If a safety property is violated, we can point to a specific point in time where the violation occurred, \textbf{the violation cannot be undone}.}
  \item{\highlight{Liveness}: ``\textit{Eventually something good happend}'': At any time, there is the chance that the property will be satisfied at a later point in time.}
\end{itemize}

\subsection{Assumptions}
\subsubsection{Why do distributed algorithms need assumptions?}
Distributed algorithms deal with a lot of uncertainty, therefore we need assumptions to describe the \textbf{uncertainty} and \textbf{guarantees} of the system. Typical are
\begin{itemize}
  \item{\textbf{Process assumptions}: Crash behavior, adherance to the protocol}
  \item{\textbf{Communication assumptions}: Topology, reliability, attackers}
  \item{\textbf{Timing assumptions}: Latency, synchrony}
  \item{\textbf{Cryptographic assumptions}: Cryptographic primitives (e.g. encryption, signatures)}
  \item{\textbf{Setup assumptions}: What information is available to the participants at the start}
\end{itemize}

\subsubsection{Uniform/Nonuniform}
\begin{itemize}
  \item{\textbf{Uniform}: Total number of processors $n$ is not known to the algorithm}
  \item{\textbf{Nonuniform}: Each processor knows the total number of processors $n$}
\end{itemize}

\subsubsection{Fault model}\label{sec:faultModel}
The \highlight{Fault model} abstracts faults in the processors and channels.
\begin{itemize}
  \item{\highlight{Crash fault}: Processor works correctly until it crashes and never recovers}
  \item{\highlight{Omission fault}: Processor fails to send/receive messages it is supposed to send/receive (e.g. due to buffer overflow)}
  \item{\highlight{Crashes with recoveries}: Either the process crashes and never recovers or the process keeps crashing and recovering infinitely often}
  \item{\highlight{Byzantine fault}: Arbitrary behavior, the process can deviate from the protocol in any way}
\end{itemize}

\subsubsection{Fault tolerance}
The \highlight{Fault tolerance} of a system is the number of faulty processes $f$ out of $n$ processes that the system can tolerate while still operating correctly.

\subsubsection{Communication: Fair-loss links}
\textbf{Fair-loss links} are defined by three properties:
\begin{enumerate}
  \item{\textit{Fair-loss}: If a correct process $p$ infinitely often sends a message $m$ to a correct process $q$, then $q$ delivers $m$ an infinite number of times}
  \item{\textit{Finite duplication}: If a correct process $p$ sends a message $m$ a finite number of times to a process $q$, then $m$ cannot be delivered an infinite number of times by $q$}
  \item{\textit{No creation}: If some process $q$ delivers a message $m$ with sender $p$, then $m$ was previously sent to $q$ by $p$}
\end{enumerate}

\subsubsection{Communication: Perfect links}\label{sec:perfectLinks}
\textbf{Perfect links} are also defined by three properties:
\begin{enumerate}
  \item{\textit{Reliable delivery}: If a correct process $p$ sends a message $m$ to a correct process $q$, then $q$ eventually delivers $m$}
  \item{\textit{No duplication}: No message is delivered by a process more than once}
  \item{\textit{No creation}: If some process $q$ delivers a message $m$ with sender $p$, then $m$ was previously sent to $q$ by $p$}
\end{enumerate}
\textbf{Authenticated perfect links} are an extension of perfect links.

\subsubsection{Timing Models}
The \highlight{Timing model} describes the timing assumptions of the communication and execution behavior:
\begin{itemize}
  \item{\highlight{Synchronous model}: There is a \textbf{known upper bound} on processing delays and on message transmission delays}
  \item{\highlight{Asynchronous model}: There is \textbf{no timing assumption at all}. The execution and message delivery happens at an arbitrary speed, \textbf{but messages arrive eventually}}
\end{itemize}

\subsection{Time in Asynchronous Systems}
\subsubsection{Logical clocks: Lamport Clocks}
\highlight{Lamport clocks} are used to \textbf{measure passage of time} in \textbf{asynchronous systems}.
\begin{itemize}
  \item{Each process $p_i$ has a \textbf{logical clock} $l_i$, initially set to $0$}
  \item{Upon an event (sending or receiving a message), $l_i$ is incremented by $1$}
  \item{When sending a message $m$, process $p_i$ adds a timestamp $t_m = l_i$ to the message}
  \item{When receiving a message $m$, process $p_j$ increases its timestamp to $\max(l_j, t_m) + 1$}
\end{itemize}
With this, a \textbf{happened-before relationship} between events is established. For any two events $e_1, e_2$: $e_1 \rightarrow e_2 \Rightarrow t(e_1) < t(e_2)$.\par
This defines a \textbf{partial order} on the events.

\subsubsection{Hybrid: Partial Synchrony}
A hybrid between synchrony and asynchrony is \textbf{partial synchrony}, which comes in two variants:
\begin{itemize}
  \item{\textbf{Eventually synchronous}/Global Stabilization Time (GST): An event GST occurs after some finite time, afterwards time bound $\Lambda$ holds}
  \item{\textbf{Unknown Latency (UL)}: The system is always synchronous, but the delay bound $\Lambda$ is unknown}
\end{itemize}
Algorithms for these models typically increment their estimation of the delay bound dynamically.

\subsection{Combining Abstractions for Assumptions}
\subsubsection{Fail-stop}
\begin{itemize}
  \item{\hyperref[sec:faultModel]{Crash faults}}
  \item{\hyperref[sec:perfectLinks]{Perfect links}}
  \item{Perfect failure detector}
\end{itemize}

\subsubsection{Fail-silent}
\begin{itemize}
  \item{\hyperref[sec:faultModel]{Crash faults}}
  \item{\hyperref[sec:perfectLinks]{Perfect links}}
  \item{No failure detector}
\end{itemize}

\subsubsection{Fail-arbitrary}
\begin{itemize}
  \item{\hyperref[sec:faultModel]{Byzantine faults}}
  \item{\hyperref[sec:perfectLinks]{Authenticated perfect links}}
\end{itemize}

\subsection{Problem Statement: Leader Election}
\subsubsection{Problem definition}
\begin{itemize}
  \item{Group of processors has to elect one of them as leader}
  \item{Exactly one processor enters an elected state, all others enter a non-elected state}
\end{itemize}

\subsubsection{Anonymous rings}
\begin{itemize}
  \item{Processors have no identifiers}
  \item{Each processor has the same deterministic state machine}
  \item{Each processor is connected to two other processors}
\end{itemize}

\subsubsection{Leader Election in Anonymous Rings}
``\textit{There is no nonuniform anonymous algorithm for leader election in synchronous rings.}''\par
So, even with these strong assumptions, \textbf{leader election is not possible with anonymous participants}.

\subsubsection{Leader Election in Asynchronous Rings}
Setup assumptions:
\begin{itemize}
  \item{Assign each processor $p_i$ a unique identifier $id_i$}
  \item{Label the two connected processors of $p_i$ as \textit{left} and \textit{right neighbor}}
\end{itemize}

\begin{algorithm}
  \caption{Algorithm for Leader Election in Asynchronous Rings}
  \begin{algorithmic}
    \State \textbf{Each processor sends its identifier to its left neighbour.}
    \State \textbf{When a processor receives a termination message, it forwards it to the left neighbour and terminates as non-leader.}\\
    \State \textbf{Upon receiving message $m$ from right neighbour}
    \State \hspace{\algorithmicindent} \textbf{if $m < id_i$ then}
    \State  \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{drop message}
    \State \hspace{\algorithmicindent} \textbf{else if $m > id_i$ then}
    \State  \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{forward $m$ to left neighbour}
    \State \hspace{\algorithmicindent} \textbf{else if $m == id_i$ then}
    \State  \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{Send termination message to left neighbour}
    \State  \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{Terminate as leader}
    \State \hspace{\algorithmicindent} \textbf{end if}
  \end{algorithmic}
\end{algorithm}

The algorithm sends not more than $O(n^2)$ messages. Nonuniforms algorithms for synchronous rings also exist.

\subsection{Problem Statement: Mutual Exclusion}
\subsubsection{Problem definition}
\textbf{Mutual exclusion} is a known problem from concurrent computing. We need to ensure than critical sections are only accessible by one processor at a time. The desired sequence is Entry $\Rightarrow$ Critical section $\Rightarrow$ Exit.\par
The following three objectives should be satisfied:
\begin{itemize}
  \item{\textbf{Mutual exclusion}: At most one process can be in the critical section at any time (\textbf{Safety})}
  \item{\textbf{No Deadlock}: If some processor enters an entry section, some processor will later enter a critical section.}
  \item{\textbf{No lockout (starvation)}: If some processor enters an entry section, \textbf{that same processor} will later enter a critical section.}
\end{itemize}

\subsubsection{Lamport Mutual Exclusion}
Lamport defined a mutual exclusion algorithm based on \textbf{logical clocks} \cite{lamportTimeClocks}, the algorithm is not covered in this summary.

\subsection{Formalization via Modules}
We can now combine abstractions using \highlight{Modules}. A module has a \textbf{name}, \textbf{events} and \textbf{safety \& liveness properties}.\par
An algorithm \textit{implements} a module (and can build on other modules).

\newpage
\subsubsection{Module: Perfect Failure Detector}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Indication}: $\langle \mathcal{P}, \textit{Crash} \mid p \rangle$: Detects that process $p$ has crashed}
\end{itemize}
\textbf{Properties}:
\begin{itemize}
  \item{\textbf{PFD1:} \textit{Strong completeness}: Eventually, every process that crashes is permanently detected by every correct process}
  \item{\textbf{PFD2:} \textit{Strong accuracy}: If a process $p$ is detected by any process, then $p$ has crashed}
\end{itemize}
Perfect Failure Detector is implemented by ``Exclude on Timeout'' (not covered in this summary).

\subsubsection{Module: Eventually Perfect Failure Detector}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Indication}: $\langle \diamond \mathcal{P}, \textit{Suspect} \mid p \rangle$: Notifies that process $p$ is suspected to have crashed}
  \item {\textbf{Indication}: $\langle \diamond \mathcal{P}, \textit{Restore} \mid p \rangle$: Notifies that process $p$ is not suspected anymore}
\end{itemize}

\textbf{Properties}:
\begin{itemize}
  \item{\textbf{EPFD1:} \textit{Strong completeness}: Eventually, every process that crashes is permanently suspected by every correct process}
  \item{\textbf{EPFD2:} \textit{Eventual strong accuracy}: Eventually, no correct process is suspected by any correct process}
\end{itemize}
Eventually Perfect Failure Detector is implemented by ``Increasing Timeout'' (not covered in this summary).

\subsubsection{Module: Leader Election}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Indication}: $\langle le, \textit{Leader} \mid p \rangle$: Indicates that process $p$ is elected as leader}
\end{itemize}

\textbf{Properties}:
\begin{itemize}
  \item{\textbf{LE1:} \textit{Eventual detection}: Either there is no correct process, or some correct process is eventually elected as the leader}
  \item{\textbf{LE2:} \textit{Accuracy}: If a process is leader, then all previously elected leaders have crashed}
\end{itemize}
Leader Election is implemented by ``Monarchical Leader Election'' (not covered in this summary).

\subsubsection{Module: Eventual Leader Detector}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Indication}: $\langle \Omega, \textit{Trust} \mid p \rangle$: Indicates that process $p$ is trusted to be leader}
\end{itemize}

\textbf{Properties}:
\begin{itemize}
  \item{\textbf{ELD1:} \textit{Eventual accuracy}: There is a time after which every correct process trusts some correct process}
  \item{\textbf{ELD2:} \textit{Eventual agreement}: There is a time after which no two correct processes trust different processes}
\end{itemize}
Eventual Leader Detector is implemented by ``Monarchical Eventual Leader Detection'' (not covered in this summary).

\subsection{Quorums}
A \highlight{Quorum} is a set of processes with special properties. They are used for fault-tolerant algorithms. Dealing with $N$ crash-fault processes, \textbf{a quorum is any majority of processes}.\par
Assumption: There is a quorum of non-faulty processes (number of faulty processes $f < N/2$).\par
\textbf{Properties}:
\begin{itemize}
  \item{Two quorums intersect in at least one process}
  \item{In every quorum is at least one correct (non-faulty) process}
\end{itemize}
Dealing with $N$ arbitrary-fault processes (byzantine): To maintaing the second property, a quorum needs to be a set of more than $\frac{N+f}{2}$ processes. We call a set of more than $\frac{N+f}{2}$ a \highlight{byzantine quorum}.\par
When the required property is that there exists a Byzantine quorum of correct processes, $3f < N$ needs to hold.

\section{Reliable Broadcast}
Reliable broadcast is used to \textbf{share information} among processors withous losses, duplicates, etc.
\subsection{Module: Best Effort Broadcast}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Request}: $\langle beb, \textit{Broadcast} \mid m \rangle$: Broadcast a message $m$ to all processes}
  \item{\textbf{Indication}: $\langle beb, \textit{Deliver} \mid p, m \rangle$: Deliver a message $m$ broadcast by process $p$}
\end{itemize}

\textbf{Properties}:
\begin{itemize}
  \item{\textbf{Liveness:}
              \begin{itemize}
                \item{\textit{Validity}: If a correct process $p$ broadcasts $m$, then every correct process eventually delivers $m$}
              \end{itemize}
        }
  \item{\textbf{Safety:}
              \begin{itemize}
                \item{\textit{No duplication}: No message is delivered more than once}
                \item{\textit{No creation}: If a process delivers a message $m$ with sender $s$, then $m$ was prev. broadcast by $s$}
              \end{itemize}}
\end{itemize}
Best Effort Broadcast is implemented by ``Best Effort Broadcast Algorithm'' (not covered in this summary).

\subsection{Module: Reliable Broadcast}
\textbf{Events}:
\begin{itemize}
  \item{\textbf{Request}: $\langle rb, \textit{Broadcast} \mid m \rangle$: Broadcast a message $m$ to all processors}
  \item{\textbf{Indication}: $\langle rb, \textit{Deliver} \mid p, m \rangle$: Deliver a message $m$ broadcast by processor $p$}
\end{itemize}

\textbf{Properties}:
\begin{itemize}
  \item{\textbf{Validity}: If a correct processor $p$ broadcasts $m$, then $p$ eventually delivers $m$}
  \item{\textbf{No duplication}: No message is delivered more than once}
  \item{\textbf{No creation}: If a processor delivers $m$ from sender $s$, then $m$ was previously broadcast by $s$}
  \item{\textbf{Agreement}: If a message $m$ is delivered by some correct processor, then $m$ is eventually delivered by every correct processor}
\end{itemize}
Reliable Broadcast is implemented by ``Reliable Broadcast Algorithm'' (not covered in this summary).

\subsection{Byzantine Reliable Broadcast: Synchronous Case}
\textbf{Problem}: Components in a distributed system don't always fail ``orderly'' (crash-fault), but they can fail in arbitrary and potentially malicious ways. Therefore we need to focus on \highlight{Byzantine failures}.

\subsubsection{Interactive Consistency}
There are $i = 1, \dots, n$ independent processors, at most $f$ of which exhibit \textbf{arbitrary faults}. Non-faulty processors have a private value $v_i$.\par
\textbf{Assumptions}:
\begin{itemize}
  \item{\textbf{A1: Reliability}: Sent messages are delivered correctly}
  \item{\textbf{A2: Authenticity}: The receiver of a message knows the sender}
  \item{\textbf{A3: Synchrony}: Absence of a message is detectable}
\end{itemize}
The goal is \highlight{Interactive Consistency}: Processors compute a vector such that
\begin{itemize}
  \item{Non-faulty processors compute a $n$-dimensional vector}
  \item{Non-faulty processors compute exactly the same vector}
  \item{The values corresponding to non-faulty processors are their private value}
  \item{The values of faulty processors can be arbitrary, but must be consistent}
\end{itemize}

\subsubsection{Challenges}
Faulty processors can lie about themselves and others:
\begin{itemize}
  \item{\highlight{Equivocation}: A processor sends different/contradicting information to different processors}
  \item{\highlight{Relay Forgery}: A processor passes on different information than received}
\end{itemize}

\subsubsection{Impossibility Result}
It can be shown that $3f$ processors are insufficient to overcome $f$ faults. \textbf{Interactive consistency} between $n$ processors can be achieve if and only if $n > 3f$ with $f$ faults.\par
\textbf{Oral Messages Algorithms} (not covered in this summary) achive interactive consistency.

\subsubsection{Authenticators}
The results seen rely on the assumptions of byzantine processors and the timing model.\par
\highlight{Authenticators} are metadata that ensure the \textbf{authenticity} and \textbf{integrity} of data (by using cryptography). They can be used to improve the results.

\subsection{Byzantine Reliable Broadcast: Asynchronous Case}
\subsubsection{Bracha's Reliable Broadcast}
\highlight{Bracha's Reliable Broadcast} \cite{BRACHA1987130} is an efficient reliable broadcast algorithm for \textbf{asynchronous} systems.\par
\textbf{Assumptions}:
\begin{itemize}
  \item{System consists of $n$ processes communicating via a \textit{reliable message system}}
  \item{The system requires $n \geq 3f + 1$ processes to tolerate $f$ byzantine faults}
\end{itemize}
\textbf{Objectives:} One sender $p$ wants to broadcast a value $v$. The algorithms satisfies the following properties:
\begin{itemize}
  \item{If $p$ is correct, then all correct processes agree on $v$}
  \item{If $p$ is faulty, then either all correct processes agree on the same value or none of them accepts any value from $p$}
\end{itemize}
Bracha's Reliable Broadcas has a message complexity of $O(n^2)$. The algoritm is not covered in this summary.

\newpage
\bibliographystyle{apalike}
\bibliography{\jobname}


\end{document}